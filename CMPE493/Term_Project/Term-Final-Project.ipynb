{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Install and Import",
   "metadata": {
    "tags": [],
    "cell_id": "00000-7d6fbbcf-2d09-4430-8d5b-fc0d762af861",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00000-89106ece-af7c-4fcb-9f7b-f4e7e5f1b4fa",
    "deepnote_cell_type": "code"
   },
   "source": "!pip install -r requirements.txt",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00000-3a3b5505-6e9d-454c-9a3b-0b22c4ad7683",
    "deepnote_cell_type": "code"
   },
   "source": "import nltk \nimport numpy as np\nimport pandas as pd\nimport pickle\nimport nltk\nimport string\nimport collections\nimport re\nfrom nltk.corpus import stopwords  \nfrom nltk.tokenize import word_tokenize \nfrom nltk.stem import PorterStemmer \nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \nimport operator\nnltk.download('stopwords')\nnltk.download('punkt')\nimport operator\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport heapq\nfrom nltk.corpus import wordnet as wn\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors \nfrom biobert_embedding.embedding import BiobertEmbedding\nfrom rank_bm25 import BM25Okapi\nfrom keybert import KeyBERT\nimport xml.etree.ElementTree as ET\n\ndef download_nltk_packages():\n    nltk.download('wordnet')\n    nltk.download('punkt')\n    nltk.download('stopwords')\n    nltk.download('averaged_perceptron_tagger')\n\ndownload_nltk_packages()    ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "metadata": {
    "tags": [],
    "cell_id": "00003-8b369a0f-240d-42f8-af20-f6f4ee867dca",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00001-4a39b2d8-f751-459c-a5eb-fa99ab033350",
    "deepnote_cell_type": "code"
   },
   "source": "#stop word keys formed\nSTOP_WORDS = set(stopwords.words('english'))\nTOPICS= {}\nCORONA_SYN = {\"corona\", \"2019-ncov\",\"sarscov-2\", \"covid-19\",\"sars-cov-2\" ,\"sars-cov2\" ,\"sars-cov\", \"covid\"}\n#stop word keys formed extra punctuated version for covid\nfor key in string.punctuation:\n    CORONA_SYN.add(\"covid\"+key+\"19\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00002-96ec5fb2-6a91-404f-93cb-cd30c3070bb4",
    "deepnote_cell_type": "code"
   },
   "source": "#stop word removal\ndef remove_stopwords(word_tokens):\n    filtered_sentence = [w for w in word_tokens if not w in STOP_WORDS]\n    return filtered_sentence\n\n#punctuation removal\ndef remove_punct(text):\n    trans_table = {key: \" \" for key in string.punctuation+\"â€™\"}\n    table = str.maketrans(trans_table)\n    text = text.translate(table)\n    return text\n    \n#synonym replace from CORONA_SYN along with other significant phrases\ndef replace_syn(text):\n    \n    newtext=text\n\n    if 'wuhan virus' in text:\n        splittext=text.split('wuhan virus')\n        newtext=splittext[0] + 'coronavirus' + splittext[1]\n\n    if 'chinese flu' in text:\n        splittext=text.split('chinese flu')\n        newtext=splittext[0] + 'coronavirus' + splittext[1]\n\n    if 'covid 19' in text:\n        splittext=text.split('covid 19')\n        newtext=splittext[0] + 'coronavirus' + splittext[1]\n    \n    if 'corono virus' in text:\n        splittext=text.split(\"corona virus\")\n        newtext=splittext[0] + 'coronavirus' + splittext[1]\n\n\n    return \" \".join([\"coronavirus\" if w in CORONA_SYN else w for w in word_tokenize(newtext)])",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00003-c1a7daa0-3e05-41d1-9a0a-b30b17b270b2",
    "deepnote_cell_type": "code"
   },
   "source": "#returns preprocess string formed by word tokens \ndef preprocess(text):\n   \n    if type(text) != str:\n        print(text)\n    else:\n        text = text.lower()\n        text = replace_syn(text)\n        text = remove_punct(text)\n        text = re.sub(r'\\d+', '', text)\n        word_tokens = word_tokenize(text)\n        word_tokens = remove_stopwords(word_tokens)\n        return \" \".join(word_tokens)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00004-547de6fe-f9c3-4ddf-a6c7-ae4852a0ad08",
    "deepnote_cell_type": "code"
   },
   "source": "#preprocess utilized on metadata.The ones come after year 2019 are significant. \n\nmetadata = pd.read_csv(\"metadata.csv\")\nmetadata = metadata[metadata[\"publish_time\"].apply( lambda x: int(x.split(\"-\")[0] ) >=2019 if not pd.isnull(x) else True) ]\nmetadata = metadata[['cord_uid', 'title', 'abstract']].copy()\nmetadata = metadata.dropna(subset=[\"abstract\", \"title\"], how='all')\nmetadata = metadata.fillna('')\nmetadata[\"abstract\"] = metadata[\"abstract\"].apply(preprocess)\nmetadata[\"title\"] = metadata[\"title\"].apply(preprocess)\nMETADATA = metadata\nMETADATA.to_pickle('metadata.p')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00005-70a547c2-dcec-40a2-bfb9-515e6534bb26",
    "deepnote_cell_type": "code"
   },
   "source": "# topics.xml read and store\n# Read the topics ( query, question annd narrative ) and store it in a dataframe\nroot = ET.parse('topics.xml').getroot()\ntopics = {}\nfor elem in root:\n    topic = {}\n    number = elem.attrib.get(\"number\")\n    children = elem.getchildren()\n    topic[\"QUERY\"]= children[0].text if children else None\n    topic[\"QUESTION\"]= children[1].text if children else None\n    topic[\"NARRATIVE\"]= children[2].text if children else None\n    if number : \n        topics[number]= topic\n\nTOPICS = pd.DataFrame.from_dict(topics, orient = \"index\")\n\n# Preprocess the topics \nTOPICS = TOPICS.applymap(preprocess)\n\nTOPICS.to_pickle(\"topics.pickle\")\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Load Preprocessed Pickles",
   "metadata": {
    "tags": [],
    "cell_id": "00009-d22ed9dc-0346-4281-8a51-ed4dcafa92ac",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00004-a91b2f23-41f3-48fe-a459-17954de73998",
    "deepnote_cell_type": "code"
   },
   "source": "#Load the corresponding  pickles\n\nwith open('topics.pickle', 'rb') as pfile:\n    TOPICS = pickle.load(pfile)\n\nwith open('metadata.p', 'rb') as pfile:\n    METADATA = pickle.load(pfile)\n\n#load word2vec model\nwv = KeyedVectors.load(\"word2vec-all-c.wordvectors\", mmap='r')\n\ntopic_scores = {}",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Query Expansion & BM25\n",
   "metadata": {
    "tags": [],
    "cell_id": "00007-bd361ab8-6b64-4c88-9869-3070e469046f",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00005-a1e6a2d6-8e07-4f78-a547-79ea8c112362",
    "deepnote_cell_type": "code"
   },
   "source": "#word2vec is used in order get similar meaning words which will be used in order to\n#extend queries\ndef get_similars(querylist):\n    similars = []\n    for word in querylist:\n        try:\n            slist = wv.wv.most_similar(positive=[word])\n            first = \"\"\n            for s in slist:\n                if s[0].casefold() != word:\n                    first = s[0].casefold()\n                    break\n            similars.append(first)\n        except:\n            print(word + \" not in vocab\")\n\n    return similars",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00006-53d84c8b-8851-4e5f-a05f-500c5f122cc8",
    "deepnote_cell_type": "code"
   },
   "source": "extended = {}\n\nfor i,topic in enumerate(TOPICS.iterrows()):\n\n  topic_num = i+1\n  print(topic_num)\n  extended[topic_num] = []\n\n  #topic query\n  tokenized_topic = topic[1]['QUERY'].split(\" \")\n  #Uncomment below if you want to apply query expansion\n  #tokenized_topic.extend(get_similars(tokenized_topic)) # x2\n  extended[topic_num].append(tokenized_topic)\n\n  #topic question\n  tokenized_topic = topic[1]['QUESTION'].split(\" \")\n  #Uncomment below if you want to apply query expansion \n  #tokenized_topic.extend(get_similars(tokenized_topic))\n  extended[topic_num].append(tokenized_topic)\n\n  #topic narrative\n  tokenized_topic = topic[1]['NARRATIVE'].split(\" \")\n  #Uncomment below if you want to apply query expansion  \n  #tokenized_topic.extend(get_similars(tokenized_topic))\n  extended[topic_num].append(tokenized_topic)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00007-fbe25257-d81f-40cc-81b5-19366b42f702",
    "deepnote_cell_type": "code"
   },
   "source": "#get bm-25 scores between title - query , question, narrative\nfor topic_num in extended.keys():\n\n    print(topic_num)\n    topic_scores[topic_num] = [] \n\n    #corpus metadata title\n    corpus = METADATA['title']\n    tokenized_corpus = corpus.apply(lambda x: x.split())\n    bm25 = BM25Okapi(tokenized_corpus)\n\n    #topic query\n    query_title_scores = bm25.get_scores(extended[topic_num][0])\n    topic_scores[topic_num].append(query_title_scores)\n\n    #topic question\n    question_title_scores = bm25.get_scores(extended[topic_num][1])\n    topic_scores[topic_num].append(question_title_scores)\n\n    #topic narrative\n    narrative_title_scores = bm25.get_scores(extended[topic_num][2])\n    topic_scores[topic_num].append(narrative_title_scores)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00008-93a2a63b-2023-4e6d-934b-30a8eafda834",
    "deepnote_cell_type": "code"
   },
   "source": "#get bm-25 scores between abstract - query , question, narrative\nfor topic_num in extended.keys():\n\n    print(topic_num)\n\n    #corpus metadata abstract\n\n    corpus = METADATA['abstract']\n    tokenized_corpus = corpus.apply(lambda x: x.split())\n    bm25 = BM25Okapi(tokenized_corpus)\n\n    #topic query\n    query_abst_scores = bm25.get_scores(extended[topic_num][0])\n    topic_scores[topic_num].append(query_abst_scores)\n\n    #topic question\n    question_abst_scores = bm25.get_scores(extended[topic_num][1])\n    topic_scores[topic_num].append(question_abst_scores)\n\n    #topic narrative\n    narrative_abst_scores = bm25.get_scores(extended[topic_num][2])\n    topic_scores[topic_num].append(narrative_abst_scores)\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00009-5e7b4586-6703-4147-a713-80f7d99682e0",
    "deepnote_cell_type": "code"
   },
   "source": "#sum up the scores ( weighted sum)\nweighted_bm25_sums = []\n# weights of the scores\nweights = [0.12, 0.11, 0.11, 0.22, 0.22, 0.22]\n\nfor key in topic_scores:\n  temp = [0]* 108620\n  \n  for i,ilist in enumerate(topic_scores[key]):\n    temp+= weights[i] * ilist\n  weighted_bm25_sums.append(temp)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Top 50 Documents",
   "metadata": {
    "tags": [],
    "cell_id": "00017-228dd454-d99e-46f1-b3b3-3df48853487d",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00014-bf5803be-959c-4b2e-8646-f3e592d815c1",
    "deepnote_cell_type": "code"
   },
   "source": "#rank the documents according to bm-25 scores and get the top X for rerank with additional steps\ntopic_dict = {}\ntopic_50 = {}\ntopic_rest = {}\n\nfor topicnum in range(1,51):\n  score_doc_dict = {}\n  for i,doc in enumerate(METADATA['cord_uid']):\n    score_doc_dict[doc] = weighted_bm25_sums[topicnum-1][i]\n  \n  topic_dict[topicnum] = sorted(score_doc_dict.items(), key=lambda x: x[1], reverse=True)\n  topic_50[topicnum] = topic_dict[topicnum] [:50]\n  topic_rest[topicnum] = topic_dict[topicnum] [50:]",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Keyword Extraction & BioBert Embedding & Cosine Similarity",
   "metadata": {
    "tags": [],
    "cell_id": "00019-71c0e7f0-2c0a-4678-9460-99a0b55eccbe",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00011-7a60abf1-fcc7-4345-a18e-7462e92a3d14",
    "deepnote_cell_type": "code"
   },
   "source": "#initialize keyword extractor and biobert embedding models\nkw_extractor = KeyBERT('distilbert-base-nli-mean-tokens')\nbiobert = BiobertEmbedding()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00015-d73b8b36-519f-49e9-8c4a-077e4bf2ce22",
    "owner_user_id": "5a895869-25fb-4cfb-b059-4bcfb010469e",
    "deepnote_cell_type": "code"
   },
   "source": "#for the top X documents for every topic, use keyword extraction to kind of summarize the documents with N words (N= 100 in this case) \n# then append these words together in a sentence\n#get the embedding for this sentence from keybert\n#rank acoridng to cosine similarities\n\nfor topic_num in topic_50: #iterate each topic\n  if int(topic_num) %2 == 0  : #only even topics are used for testing\n    topic_top_embedding = [] \n    #join query - question and narrative of the topic\n    topic_query = \" \".join(extended[topic_num][0] + extended[topic_num][1] + extended[topic_num][2])\n    # treat it as a sentence and get embedding\n    topic_embedding = biobert.sentence_vector(topic_query).numpy()\n    for doc_score in topic_50[topic_num]:\n        #keyword extraction\n        keyword_tuples = kw_extractor.extract_keywords( METADATA.loc[METADATA[\"cord_uid\"] == doc_score[0], \"abstract\"].item(),keyphrase_ngram_range =(1,1),stop_words=None,top_n=100)\n        keywords = [keytuple[0] for keytuple in keyword_tuples]\n        #join keywords to form a sentence\n        keyword_sentence = \" \".join(keywords)\n        #get embedding\n        sentence_embedding = biobert.sentence_vector(keyword_sentence)\n        topic_top_embedding.append(sentence_embedding.numpy())\n  \n    try: \n        #calculate cosine similarities for the newly calculated embeddings of the documents\n        cosine_similarities = cosine_similarity([topic_embedding], topic_top_embedding).flatten()\n        cosine_similarities = enumerate(cosine_similarities)\n        cosine_similarities = sorted(cosine_similarities, key=operator.itemgetter(1), reverse= True)\n        #rank\n        sorted_doc_ids = [(topic_50[topic_num][index][0] , score)   for index, score in cosine_similarities]\n        with open('top_doc_' + str(topic_num)+'.pickle', 'wb') as topicpickle:\n            pickle.dump(sorted_doc_ids, topicpickle, pickle.HIGHEST_PROTOCOL)\n    except:\n        print(topic_num)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Output to test file",
   "metadata": {
    "tags": [],
    "cell_id": "00022-67d9f7ee-9dd9-4024-9e6f-a4df4103bfa4",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00016-4866e120-5fab-4702-842c-5409e7686c9e",
    "deepnote_cell_type": "code"
   },
   "source": "#read similarity scores of first 50 documents\nfor i in range(2,51,2):\n  with open('top_doc_' + str(i)+'.pickle', 'rb') as allpickle:\n      top_d = pickle.load(allpickle) \n      top_50_sim[i]= top_d\n\n#for all even topics, output first 50 document's scores, then the rest of the documents\nwith open('final.test', 'w') as outfile:\n  for topic in topic_dict.keys():\n    if int(topic) %2 == 0:\n      for i,tup in enumerate(top_50_sim[topic]):\n          #25 is added to sync cosine similarity results with BM scores. Max BM score is 25.\n          s= str(topic) + \" Q0 \" + str(tup[0]) + \" \" + str(i+1) + \" \" + str(tup[1] + 25) + \" GAM-run1\" + \"\\n\"\n          outfile.write(s)\n      for i,tup in enumerate(topic_rest[topic]):\n          s= str(topic) + \" Q0 \" + str(tup[0]) + \" \" + str(i+101) + \" \" + str(tup[1]) + \" GAM-run1\" + \"\\n\"\n          outfile.write(s)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Word2Vec Model Generator\n",
   "metadata": {
    "tags": [],
    "cell_id": "00015-502cc12e-6ac5-4d86-87be-819804f3d5a0",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00015-bea4bf8f-fc09-4384-8106-550ab424be4f",
    "deepnote_cell_type": "code"
   },
   "source": "#prepare data\ntexts =METADATA['abstract'].tolist()\ntexts.extend(METADATA['title'].tolist())\ntexts.extend(TOPICS[\"QUERY\"].tolist())\ntexts.extend(TOPICS[\"NARRATIVE\"].tolist())\ntexts.extend(TOPICS[\"QUESTION\"].tolist())",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00016-cc5e6ed7-8942-428a-8180-44a7bbd3503a",
    "deepnote_cell_type": "code"
   },
   "source": "#preprocess and split into tokens to be able to feed it into the model training\nfor ii in range(len(texts)):\n    try:\n        st = texts[ii]\n        st = st.lower()\n        sent = tokenize.sent_tokenize(st)\n        if sent:\n            sentences = [re.sub(pattern=r'[\\!\"#$%&\\*+,-./:;<=>?@^_`()|~=]', \n                                repl='', \n                                string=x\n                            ).strip().split(' ') for x in sent]\n            sentences = [x for x in sentences if x != ['']]\n            processed.append(sentences)\n    except:\n        print(ii)\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00017-ab6d53b7-5d58-4353-bc97-3656378b9422",
    "deepnote_cell_type": "code"
   },
   "source": "#merge sentences together\nall_sentences = []\nfor p in processed:\n    all_sentences.extend(p)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00018-40524408-555d-4305-a811-81402d9194b8",
    "deepnote_cell_type": "code"
   },
   "source": "#train word2vec model\nmodel = Word2Vec(all_sentences, \n                 min_count=2,   # Ignore words that appear less than this\n                 size=200,      # Dimensionality of word embeddings\n                 workers=2,     # Number of processors (parallelisation)\n                 window=6,      # Context window for words during training\n                 iter=30)  ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00019-6263a5d7-99d1-4378-8217-fcb4b745af6f",
    "deepnote_cell_type": "code"
   },
   "source": "word_vectors = model.wv",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00025-bbbbc396-f32d-4e80-a011-1ccf8684c334",
    "deepnote_cell_type": "code"
   },
   "source": "#save word2vec model\nword_vectors.save(\"word2vec-all-c.wordvectors\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=c30274b2-cddd-4d19-919a-3ecdb6dd5b3f' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote_notebook_id": "d07361b7-2268-4c72-9b23-8af5146b1b0c",
  "deepnote": {},
  "deepnote_execution_queue": []
 }
}